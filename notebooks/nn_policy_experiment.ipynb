{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4f885f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import gymnasium as gym\n",
    "import tqdm\n",
    "\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95bc3e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(5,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid')) #left probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c420397d",
   "metadata": {},
   "source": [
    "#### CREDIT ASSIGNMENT\n",
    "\n",
    "`We're trying to give weightage to the action which gave more rewards. Kind of weightage rewards. We do this by mutliplying the rewards by dicounted rate and adding it to the previous sum od rewards`\n",
    "\n",
    "`Discounted Rate (Gamma) : [0.9,0.99] Mostly`\n",
    "\n",
    "In Pole example, one good reward step followed by bad rewards step can make the pole fall. So we need to assign more importance to the good step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d193c1b9",
   "metadata": {},
   "source": [
    "#### Policy Gradients\n",
    "` Optimize learnable parameters of policy by following the gradients towards higher reward.`\n",
    "\n",
    "#### Steps\n",
    "\n",
    "`Step 1: let the nn play its game multiple timesand at every step just calculate the gradiets (wrt reward) but don't apply it immidiately.`\n",
    "\n",
    "`Step 2: Once you have completed several episodes then compute the actions using discounted method. `\n",
    "\n",
    "`Step 3: Result of previous step 2 can be poisitive and negative. `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2714a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pg_policy(observation, model):\n",
    "    left_prob = model.predict(observation[np.newaxis]) # prob 0-1\n",
    "    action = int(np.random.rand()>left_prob) # exploration vs exploitation concept\n",
    "    # to force the algo to go for the other option to explore\n",
    "    return action\n",
    "\n",
    "def play_one_step(env, observations, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_prob = model(observations[np.newaxis]) # --> Predicted\n",
    "        action = (tf.random.uniform([1,1])>left_prob) # True or False [0 if left, 1 if right]\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32) # y_target is the prob of left that should be 1. So, 1 - action (float32)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_prob)) # --> (y_true,y_pred)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables) #dc/dw\n",
    "    new_observations, reward, done, info, _ = env.step(int(action))\n",
    "    return new_observations, reward, done, grads\n",
    "\n",
    "def play_multiple_episdoes(env, N_episodes, N_steps, model, loss_fn):\n",
    "    total_rewards = list()\n",
    "    total_grads = list()\n",
    "    \n",
    "    for episode in range(N_episodes):\n",
    "        current_rewards = list()\n",
    "        current_grads = list()\n",
    "        observation, info = env.reset()  # observation : [CartPosition, CartVelocity, PoleAngle, PoleAngularVelocity]\n",
    "        for step in range(N_steps):\n",
    "            new_observations, reward, done, grads = play_one_step(env, observation, model, loss_fn)\n",
    "            env.render()\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break\n",
    "        total_rewards.append(current_rewards)\n",
    "        total_grads.append(current_grads)\n",
    "    \n",
    "    return total_rewards, total_grads\n",
    "\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    N = len(rewards)\n",
    "    for step in range(N -2, -1, -1):\n",
    "        discounted[step] = discounted[step] + discounted[step + 1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(total_rewards, discount_factor):\n",
    "    total_discounted_rewards = list()\n",
    "    for rewards in total_rewards:\n",
    "        total_discounted_rewards.append(discount_rewards(rewards,discount_factor))\n",
    "    \n",
    "    flat_rewards = np.concatenate(total_discounted_rewards)\n",
    "    reward_mean, reward_std = flat_rewards.mean(), flat_rewards.std()\n",
    "    \n",
    "    normalized_discounted_rewards = list()\n",
    "    for discounted_reward in total_discounted_rewards:\n",
    "        nrs = (discounted_reward - reward_mean)/reward_std\n",
    "        normalized_discounted_rewards.append(nrs)\n",
    "        \n",
    "    return normalized_discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0431a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_episodes = 10\n",
    "N_steps = 150\n",
    "N_max_steps = 200\n",
    "discount_factor = 0.95\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a55164f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1/150 Mean Rewards: 22.85\n",
      "Iteration: 2/150 Mean Rewards: 24.8\n",
      "Iteration: 3/150 Mean Rewards: 19.15\n",
      "Iteration: 4/150 Mean Rewards: 22.95\n",
      "Iteration: 5/150 Mean Rewards: 19.9\n",
      "Iteration: 6/150 Mean Rewards: 19.6\n",
      "Iteration: 7/150 Mean Rewards: 27.1\n",
      "Iteration: 8/150 Mean Rewards: 22.05\n",
      "Iteration: 9/150 Mean Rewards: 21.85\n",
      "Iteration: 10/150 Mean Rewards: 19.8\n",
      "Iteration: 11/150 Mean Rewards: 21.4\n",
      "Iteration: 12/150 Mean Rewards: 21.4\n",
      "Iteration: 13/150 Mean Rewards: 21.6\n",
      "Iteration: 14/150 Mean Rewards: 21.75\n",
      "Iteration: 15/150 Mean Rewards: 18.75\n",
      "Iteration: 16/150 Mean Rewards: 21.6\n",
      "Iteration: 17/150 Mean Rewards: 26.4\n",
      "Iteration: 18/150 Mean Rewards: 22.2\n",
      "Iteration: 19/150 Mean Rewards: 22.45\n",
      "Iteration: 20/150 Mean Rewards: 30.15\n",
      "Iteration: 21/150 Mean Rewards: 22.5\n",
      "Iteration: 22/150 Mean Rewards: 21.5\n",
      "Iteration: 23/150 Mean Rewards: 26.45\n",
      "Iteration: 24/150 Mean Rewards: 20.7\n",
      "Iteration: 25/150 Mean Rewards: 23.5\n",
      "Iteration: 26/150 Mean Rewards: 21.35\n",
      "Iteration: 27/150 Mean Rewards: 21.2\n",
      "Iteration: 28/150 Mean Rewards: 19.3\n",
      "Iteration: 29/150 Mean Rewards: 24.45\n",
      "Iteration: 30/150 Mean Rewards: 25.05\n",
      "Iteration: 31/150 Mean Rewards: 23.9\n",
      "Iteration: 32/150 Mean Rewards: 20.55\n",
      "Iteration: 33/150 Mean Rewards: 22.0\n",
      "Iteration: 34/150 Mean Rewards: 25.6\n",
      "Iteration: 35/150 Mean Rewards: 27.0\n",
      "Iteration: 36/150 Mean Rewards: 30.5\n",
      "Iteration: 37/150 Mean Rewards: 26.45\n",
      "Iteration: 38/150 Mean Rewards: 26.5\n",
      "Iteration: 39/150 Mean Rewards: 22.3\n",
      "Iteration: 40/150 Mean Rewards: 23.25\n",
      "Iteration: 41/150 Mean Rewards: 20.25\n",
      "Iteration: 42/150 Mean Rewards: 25.1\n",
      "Iteration: 43/150 Mean Rewards: 21.55\n",
      "Iteration: 44/150 Mean Rewards: 21.7\n",
      "Iteration: 45/150 Mean Rewards: 17.65\n",
      "Iteration: 46/150 Mean Rewards: 25.95\n",
      "Iteration: 47/150 Mean Rewards: 20.9\n",
      "Iteration: 48/150 Mean Rewards: 22.55\n",
      "Iteration: 49/150 Mean Rewards: 25.75\n",
      "Iteration: 50/150 Mean Rewards: 22.55\n",
      "Iteration: 51/150 Mean Rewards: 18.85\n",
      "Iteration: 52/150 Mean Rewards: 20.75\n",
      "Iteration: 53/150 Mean Rewards: 20.05\n",
      "Iteration: 54/150 Mean Rewards: 19.5\n",
      "Iteration: 55/150 Mean Rewards: 23.5\n",
      "Iteration: 56/150 Mean Rewards: 22.95\n",
      "Iteration: 57/150 Mean Rewards: 18.1\n",
      "Iteration: 58/150 Mean Rewards: 23.95\n",
      "Iteration: 59/150 Mean Rewards: 23.2\n",
      "Iteration: 60/150 Mean Rewards: 23.95\n",
      "Iteration: 61/150 Mean Rewards: 23.1\n",
      "Iteration: 62/150 Mean Rewards: 19.35\n",
      "Iteration: 63/150 Mean Rewards: 21.2\n",
      "Iteration: 64/150 Mean Rewards: 21.5\n",
      "Iteration: 65/150 Mean Rewards: 22.2\n",
      "Iteration: 66/150 Mean Rewards: 17.45\n",
      "Iteration: 67/150 Mean Rewards: 23.6\n",
      "Iteration: 68/150 Mean Rewards: 21.05\n",
      "Iteration: 69/150 Mean Rewards: 24.05\n",
      "Iteration: 70/150 Mean Rewards: 22.25\n",
      "Iteration: 71/150 Mean Rewards: 24.45\n",
      "Iteration: 72/150 Mean Rewards: 20.3\n",
      "Iteration: 73/150 Mean Rewards: 24.0\n",
      "Iteration: 74/150 Mean Rewards: 23.55\n",
      "Iteration: 75/150 Mean Rewards: 22.4\n",
      "Iteration: 76/150 Mean Rewards: 22.95\n",
      "Iteration: 77/150 Mean Rewards: 21.35\n",
      "Iteration: 78/150 Mean Rewards: 23.7\n",
      "Iteration: 79/150 Mean Rewards: 27.3\n",
      "Iteration: 80/150 Mean Rewards: 19.4\n",
      "Iteration: 81/150 Mean Rewards: 23.5\n",
      "Iteration: 82/150 Mean Rewards: 24.9\n",
      "Iteration: 83/150 Mean Rewards: 22.1\n",
      "Iteration: 84/150 Mean Rewards: 23.35\n",
      "Iteration: 85/150 Mean Rewards: 22.2\n",
      "Iteration: 86/150 Mean Rewards: 23.75\n",
      "Iteration: 87/150 Mean Rewards: 23.5\n",
      "Iteration: 88/150 Mean Rewards: 16.5\n",
      "Iteration: 89/150 Mean Rewards: 19.2\n",
      "Iteration: 90/150 Mean Rewards: 22.95\n",
      "Iteration: 91/150 Mean Rewards: 21.45\n",
      "Iteration: 92/150 Mean Rewards: 19.65\n",
      "Iteration: 93/150 Mean Rewards: 26.3\n",
      "Iteration: 94/150 Mean Rewards: 28.25\n",
      "Iteration: 95/150 Mean Rewards: 20.8\n",
      "Iteration: 96/150 Mean Rewards: 19.7\n",
      "Iteration: 97/150 Mean Rewards: 23.35\n",
      "Iteration: 98/150 Mean Rewards: 20.15\n",
      "Iteration: 99/150 Mean Rewards: 23.0\n",
      "Iteration: 100/150 Mean Rewards: 23.15\n",
      "Iteration: 101/150 Mean Rewards: 25.65\n",
      "Iteration: 102/150 Mean Rewards: 21.55\n",
      "Iteration: 103/150 Mean Rewards: 20.55\n",
      "Iteration: 104/150 Mean Rewards: 21.3\n",
      "Iteration: 105/150 Mean Rewards: 25.75\n",
      "Iteration: 106/150 Mean Rewards: 20.45\n",
      "Iteration: 107/150 Mean Rewards: 18.9\n",
      "Iteration: 108/150 Mean Rewards: 22.75\n",
      "Iteration: 109/150 Mean Rewards: 22.05\n",
      "Iteration: 110/150 Mean Rewards: 22.5\n",
      "Iteration: 111/150 Mean Rewards: 21.85\n",
      "Iteration: 112/150 Mean Rewards: 20.4\n",
      "Iteration: 113/150 Mean Rewards: 18.05\n",
      "Iteration: 114/150 Mean Rewards: 22.45\n",
      "Iteration: 115/150 Mean Rewards: 19.9\n",
      "Iteration: 116/150 Mean Rewards: 21.25\n",
      "Iteration: 117/150 Mean Rewards: 19.7\n",
      "Iteration: 118/150 Mean Rewards: 19.45\n",
      "Iteration: 119/150 Mean Rewards: 19.0\n",
      "Iteration: 120/150 Mean Rewards: 20.15\n",
      "Iteration: 121/150 Mean Rewards: 16.45\n",
      "Iteration: 122/150 Mean Rewards: 24.4\n",
      "Iteration: 123/150 Mean Rewards: 27.25\n",
      "Iteration: 124/150 Mean Rewards: 17.6\n",
      "Iteration: 125/150 Mean Rewards: 20.0\n",
      "Iteration: 126/150 Mean Rewards: 20.3\n",
      "Iteration: 127/150 Mean Rewards: 20.35\n",
      "Iteration: 128/150 Mean Rewards: 19.3\n",
      "Iteration: 129/150 Mean Rewards: 20.6\n",
      "Iteration: 130/150 Mean Rewards: 22.0\n",
      "Iteration: 131/150 Mean Rewards: 24.0\n",
      "Iteration: 132/150 Mean Rewards: 20.45\n",
      "Iteration: 133/150 Mean Rewards: 24.25\n",
      "Iteration: 134/150 Mean Rewards: 21.1\n",
      "Iteration: 135/150 Mean Rewards: 20.35\n",
      "Iteration: 136/150 Mean Rewards: 26.6\n",
      "Iteration: 137/150 Mean Rewards: 19.2\n",
      "Iteration: 138/150 Mean Rewards: 20.7\n",
      "Iteration: 139/150 Mean Rewards: 19.4\n",
      "Iteration: 140/150 Mean Rewards: 22.7\n",
      "Iteration: 141/150 Mean Rewards: 22.55\n",
      "Iteration: 142/150 Mean Rewards: 27.8\n",
      "Iteration: 143/150 Mean Rewards: 23.15\n",
      "Iteration: 144/150 Mean Rewards: 22.45\n",
      "Iteration: 145/150 Mean Rewards: 20.25\n",
      "Iteration: 146/150 Mean Rewards: 17.85\n",
      "Iteration: 147/150 Mean Rewards: 21.05\n",
      "Iteration: 148/150 Mean Rewards: 20.7\n",
      "Iteration: 149/150 Mean Rewards: 21.55\n",
      "Iteration: 150/150 Mean Rewards: 20.6\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset(seed=SEED)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_fn = tf.keras.losses.binary_crossentropy\n",
    "\n",
    "for iteration in range(N_steps):\n",
    "    total_rewards, total_grads = play_multiple_episdoes(env, N_episodes, N_max_steps, model, loss_fn)\n",
    "    sum_total_rewards = sum(map(sum,total_rewards))\n",
    "    print(f\"Iteration: {iteration+1}/{N_steps}\",\n",
    "         f\"Mean Rewards: {sum_total_rewards/N_episodes}\")\n",
    "    \n",
    "    total_final_rewards = discount_and_normalize_rewards(total_rewards, discount_factor)\n",
    "    total_mean_grads = list()\n",
    "    \n",
    "    # Weights for 5 hidden nodes, bias for 5 nodes, w for output nodes, bias for output node\n",
    "    \n",
    "    N = len(model.trainable_variables)\n",
    "    for var_index in range(N):\n",
    "        temp_reduce_mean = list()\n",
    "        for episode_index, final_rewards in enumerate(total_final_rewards):\n",
    "            for step, final_reward in enumerate(final_rewards):\n",
    "                result = final_reward * total_grads[episode_index][step][var_index]\n",
    "                temp_reduce_mean.append(result)\n",
    "        mean_grads = tf.reduce_mean(temp_reduce_mean, axis=0)\n",
    "        total_mean_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(total_mean_grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f1ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "unique_name = re.sub(r\"[\\s+:]\",\"_\",time.asctime())\n",
    "model_name = f\"model_{unique_name}.h5\"\n",
    "model.save(model_name)\n",
    "print(\"Model saved as {}\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff1562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_one_episode(policy, model, N_steps=500, seed=42):\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    obs, info = env.reset()\n",
    "    for step in range(N_steps):\n",
    "        env.render()\n",
    "        action = policy(obs, model)\n",
    "        Observations, reward, done, info, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return step, Observations\n",
    "\n",
    "show_one_episode(pg_policy, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee144ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
